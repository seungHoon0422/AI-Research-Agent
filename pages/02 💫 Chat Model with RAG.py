import streamlit as st
from azure.core.exceptions import HttpResponseError, ClientAuthenticationError, ResourceNotFoundError
from dotenv import load_dotenv
import json
from core.tool_calling import ToolCallingManager
from models.model_selector import model, init_model
from prompt.system_prompt import DEFAULT_SYSTEM_PROMPT
from information.button import RAG_CHAT_HELP_BUTTON_MESSAGE, RAG_CHAT_CLEAR_BUTTON_MESSAGE, RAG_CHAT_HELP_DIALOG_MESSAGE
from information.title import RAG_CHAT_TITLE
import os

st.set_page_config(page_title="AI ê¸°ìˆ  ë¦¬ì„œì¹˜ í”Œë«í¼", page_icon="ğŸ¤–", layout="wide")


#### NOTE: title ì„¤ì •
col2_1, col2_2 = st.columns([0.8, 0.2])
with col2_1:
    st.markdown(RAG_CHAT_TITLE)
    # ì—ì´ì „íŠ¸ ì‚¬ìš©ë²• ëª¨ë‹¬ í•¨ìˆ˜ ì •ì˜

with col2_2:
    # ì—ì´ì „íŠ¸ ì‚¬ìš©ë²• ëª¨ë‹¬ ë²„íŠ¼
    help_button_clicked = st.button(RAG_CHAT_HELP_BUTTON_MESSAGE, use_container_width=True, help="ì—ì´ì „íŠ¸ ì‚¬ìš©ë²• í™•ì¸")
    
    # ë²„íŠ¼ í´ë¦­ ì‹œ ëª¨ë‹¬ í˜¸ì¶œ
    if help_button_clicked:
        from information.dialog import show_rag_chat_help_dialog
        show_rag_chat_help_dialog()
    
    # ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼ (ì•„ë˜ë¡œ ì´ë™)
    chat_clear_button_clicked = st.button(RAG_CHAT_CLEAR_BUTTON_MESSAGE, use_container_width=True, help="ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”")
    if chat_clear_button_clicked:
        st.session_state.messages = []
        st.toast("ì±„íŒ… ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.rerun()

st.markdown("---")


#### NOTE: Sidebar model Config ì„¤ì •
st.sidebar.title("ğŸ’¡ ëª¨ë¸ ì„¤ì •")


### NOTE: ëª¨ë¸ ì±„íŒ…

load_dotenv()

### ëª¨ë¸ ì´ˆê¸°í™”
init_model()
#### ì±„íŒ… ê¸°ë¡ì˜ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

#### SYTEM Prompt ì´ˆê¸°í™”
if st.session_state.messages == []:
    st.session_state.messages.append({"role": "system", "content": DEFAULT_SYSTEM_PROMPT})

# RAG ì—ì´ì „íŠ¸ ì±„íŒ…ì—ì„œëŠ” ë„êµ¬ ì‚¬ìš© x
# use_tool = st.sidebar.checkbox("ğŸ”§ ë„êµ¬ ì‚¬ìš©", value=False, help="ë„êµ¬ ì‚¬ìš© ì—¬ë¶€")
use_tool = False

####  NOTE: openai client ì„¤ì •

model_client = model.client()
temperature = st.sidebar.slider("temperature", 0.0, 1.0, 0.4)

tool_calling_manager = None
if use_tool:
    tool_calling_manager = ToolCallingManager(azure_client=model_client, model_name=model.base_model_name())


    

#### Azure OpenAI Client ìƒì„±
def get_azure_openai_client(messages):
    tools =[]
    if use_tool:
        tools = tool_calling_manager.tool_list

    try:
        rag_params={
                "data_sources": [
                    {
                        "type": "azure_search",
                        "parameters": {
                            "endpoint": os.getenv("AZURE_AI_SEARCH_ENDPOINT"),
                            "index_name": os.getenv("INDEX_NAME", "aisearch"),
                            "authentication": {
                                "type": "api_key",
                                "key": os.getenv("AZURE_AI_SEARCH_API_KEY")
                            }
                        }
                    }
                ]
            }
            
        response = model.chat(
            tools=tools,
            messages=messages,
            temperature=temperature,
            extra_body=rag_params,
        )

        final_response = None

        # Tool í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
        if response.choices[0].message.tool_calls:
            # Tool í˜¸ì¶œì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            # ChatCompletionMessage ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            assistant_message = {
                "role": "assistant",
                "content": response.choices[0].message.content or "",
                "tool_calls": [
                    {
                        "id": tool_call.id,
                        "type": tool_call.type,
                        "function": {
                            "name": tool_call.function.name,
                            "arguments": tool_call.function.arguments
                        }
                    } for tool_call in response.choices[0].message.tool_calls
                ]
            }
            # pprint(f"Assistant Message: {assistant_message}")
            messages.append(assistant_message)
            
            tool_response,tool_response_str = None, None

            for tool_call in response.choices[0].message.tool_calls:
                tool_name = tool_call.function.name
                tool_args = json.loads(tool_call.function.arguments)
                print(f"Tool {tool_name} called with arguments: {tool_args}")
                
                # Tool ì‹¤í–‰
                tool_response_str = ""
                with st.spinner(f'Tool {tool_name} is calling...'):
                    try:
                        tool_response = tool_calling_manager.execute_tool_call(tool_call, tool_name, tool_args)
                        tool_response_str = json.dumps(tool_response, default=str, ensure_ascii=False)
                        # UIì— í‘œì‹œ (expander ì‚¬ìš©)
                        with st.chat_message("assistant"):
                            with st.expander(f'Tool Calling: {tool_name}', expanded=False):
                                st.write(tool_response_str)
                    except Exception as e:
                        st.error(f"Tool {tool_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                        tool_response_str = json.dumps({"error": str(e)})
                                
                # Tool ì‘ë‹µì„ ë©”ì‹œì§€ì— ì¶”ê°€
                tool_message = {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": tool_response_str,
                }
                messages.append(tool_message)
            # ìµœì¢… ì‘ë‹µ ìƒì„±
            final_response = model.chat(
                messages=messages,
                temperature=temperature,
                extra_body=rag_params,
            )

        # Tool í˜¸ì¶œì´ ì—†ëŠ” ê²½ìš°
        else:
            final_response = response
        
        # ì°¸ê³ í•œ ì²­í¬(citations) í‘œì‹œ (ë©”ì‹œì§€ ë‚´ ë§ˆì»¤ [docN] ê¸°ë°˜ í•„í„°ë§)
        try:
            import re
            msg = final_response.choices[0].message
            ctx = getattr(msg, "context", None)
            content_text = msg.content or ""
            doc_markers = set(int(m) for m in re.findall(r"\[doc(\d+)\]", content_text))
            if isinstance(ctx, dict):
                all_citations = ctx.get("citations", []) or []
                # [docN] ë§ˆì»¤ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì¸ë±ìŠ¤ë§Œ(1-based) í•„í„°ë§, ì—†ìœ¼ë©´ ì „ì²´ ë…¸ì¶œ(í˜¹ì€ ìƒìœ„ 3ê°œ ë“±)
                if doc_markers:
                    citations = [c for i, c in enumerate(all_citations, start=1) if i in doc_markers]
                else:
                    citations = all_citations
                if citations:
                    with st.chat_message("assistant"):
                        with st.expander("ğŸ”— ì°¸ê³ í•œ ë¬¸ì„œ/ì²­í¬", expanded=False):
                            for idx, cite in enumerate(citations, start=1):
                                title = cite.get("title") or cite.get("filepath") or f"Citation {idx}"
                                url = cite.get("url")
                                filepath = cite.get("filepath")
                                chunk_id = cite.get("chunk_id")
                                # í‘œì‹œìš© íŒŒì¼ëª… (filepathê°€ ìˆìœ¼ë©´ basename ì‚¬ìš©)
                                try:
                                    filename = os.path.basename(filepath) if filepath else title
                                except Exception:
                                    filename = title
                                content = cite.get("content", "")
                                if content:
                                    exp_title = f"ë¯¸ë¦¬ë³´ê¸° - {filename} (chunkid: {chunk_id})" if chunk_id is not None else f"ë¯¸ë¦¬ë³´ê¸° - {filename}"
                                    with st.expander(exp_title, expanded=False):
                                        st.text(content)
        except Exception:
            pass

        return final_response.choices[0].message.content

    except (HttpResponseError, ClientAuthenticationError, ResourceNotFoundError) as e:
        st.error(f"Azure OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return f"ì˜¤ë¥˜: {e}" 
    except Exception as e:
        st.error(f"OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return f"ì˜¤ë¥˜: {e}"



#### NOTE: ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ 
for message in st.session_state.messages:
    if not message:
        continue
    elif message["role"] == "system":
        continue
    elif message["role"] == "tool":
        # íˆ´ í˜¸ì¶œ ê²°ê³¼ í‘œì‹œ
        with st.chat_message("assistant"):
            with st.expander(f'Tool Calling: {message.get("name", "unknown")}', expanded=False):
                st.write(message["content"])
    elif message.get("tool_calls"):
        # íˆ´ í˜¸ì¶œ ìš”ì²­ ë©”ì‹œì§€ëŠ” í‘œì‹œí•˜ì§€ ì•Šê±°ë‚˜ ê°„ë‹¨íˆ í‘œì‹œ
        continue
    else:  # ì¼ë°˜ ë©”ì‹œì§€ì¸ ê²½ìš°
        st.chat_message(message["role"]).write(message["content"])


#### NOTE ì‚¬ìš©ì ì±„íŒ… ì…ë ¥ ë° chat message ì¶”ê°€
if user_input := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):

    st.session_state.messages.append({"role": "user","content": user_input})
    st.chat_message("user").markdown(user_input)

    with st.spinner("GPTê°€ ì‘ë‹µí•˜ëŠ” ì¤‘..."):
        assistant_response = get_azure_openai_client(st.session_state.messages)
    
    st.session_state.messages.append({"role": "assistant", "content": assistant_response})
    st.chat_message("assistant").markdown(assistant_response)