import streamlit as st
import pandas as pd
import random
import math
from datetime import datetime, timedelta
from streamlit_agraph import agraph, Node, Edge, Config

# í˜ì´ì§€ ì „ì²´í™”ë©´ ì„¤ì •
st.set_page_config(page_title="ğŸ€ Document Hub", page_icon="ğŸ—ºï¸", layout="wide")

# í‚¤ì›Œë“œ ê¸°ë°˜ AI ê¸°ìˆ  ê´€ê³„ë„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
# ê²€ìƒ‰ëœ ìë£Œì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ì§€ì‹ í—ˆë¸Œì˜ Document keyword ê¸°ë°˜ ê´€ê³„ë„ ìƒì„±í•©ë‹ˆë‹¤.
# í˜„ì¬ëŠ” ë°ì´í„°ê°€ ì—°ê²°ë˜ì–´ìˆì§€ ì•Šì€ ìƒíƒœë¡œ, dummy ë°ì´í„°ë¡œ ë…¸ì¶œ
# í‚¤ì›Œë“œëŠ” Vector, Model, Search, Database, Framework ë“±ìœ¼ë¡œ ì„ì‹œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , í•˜ìœ„ ê¸°ìˆ ì„ 3,4ê°œ ì”© ì˜ˆì‹œ ë…¼ë¬¸ ì´ë¦„ì„ ì…ë ¥í•´ì„œ ë°ì´í„° ìƒì„±

# ì‚¬ìš©ìì—ê²Œ ë…¸ì¶œë  ëŒ€ì‹œë³´ë“œëŠ” ì°¨íŠ¸ì™€ í‘œ í˜•ì‹
# ì°¨íŠ¸ì—ëŠ” ë“±ë¡ë˜ì–´ìˆëŠ” í‚¤ì›Œë“œ í•˜ìœ„ì˜ ê¸°ìˆ  ê°œìˆ˜ë¥¼ í•œëˆˆì— ë³¼ ìˆ˜ ìˆê²Œ ìƒì„±
# í‘œì—ëŠ” ì „ì²´ í‚¤ì›Œë“œ ì„ íƒ, í‚¤ì›Œë“œë³„ ì„ íƒ ê¸°ëŠ¥ì„ í†µí•´ì„œ í•´ë‹¹ ê¸°ìˆ ì˜ ë…¼ë¬¸ ì´ë¦„ì„ í‘œì‹œ

# Title ì„¤ì •
col_title_1, col_title_2 = st.columns([0.8, 0.2])
with col_title_1:
    st.markdown('# ğŸ€ Document Hub')
with col_title_2:
    # ì§€ì‹ í—ˆë¸Œ ê´€ë ¨ Information Dialog ë²„íŠ¼
    from information.dialog import show_knowledge_hub_info_dialog
    from information.button import DOCUMENT_HUB_HELP_BUTTON_MESSAGE
    info_button_clicked = st.button(DOCUMENT_HUB_HELP_BUTTON_MESSAGE, use_container_width=True, help="Document Hub ì‚¬ìš©ë²•")
    if info_button_clicked:
        show_knowledge_hub_info_dialog()

st.markdown("---")

# ëœë¤ ì‹œë“œ ì„¤ì • (ì¼ê´€ì„± ìœ ì§€)
random.seed(42)

# ì¹´í…Œê³ ë¦¬ë³„ í•˜ìœ„ í‚¤ì›Œë“œ (ì¹´í…Œê³ ë¦¬ì— ë§ê²Œ ë¶„ë¥˜)
category_keywords = {
    "Vector": [
        "Embedding", "Vector DB", "FAISS", "Pinecone", "Weaviate",
        "Semantic Search", "Similarity", "Indexing", "Retrieval", "ANN"
    ],
    "Model": [
        "LLM", "GPT", "BERT", "Transformer", "Fine-tuning",
        "Training", "Inference", "Optimization", "Quantization", "Deployment"
    ],
    "Search": [
        "Full-text", "Keyword", "Fuzzy", "Semantic", "Hybrid",
        "Ranking", "Relevance", "Query", "Index", "Elasticsearch"
    ],
    "Database": [
        "PostgreSQL", "MongoDB", "Redis", "SQL", "NoSQL",
        "Sharding", "Replication", "Backup", "Performance", "ACID"
    ],
    "Framework": [
        "LangChain", "LlamaIndex", "Hugging Face", "PyTorch", "TensorFlow",
        "Streamlit", "FastAPI", "Flask", "Django", "Next.js"
    ]
}

# ê¸°ìˆ  ê°œìˆ˜ë¥¼ 1~10 ì‚¬ì´ ëœë¤ìœ¼ë¡œ ìƒì„±
def generate_random_count():
    return random.randint(1, 10)

# ëœë¤ ë‚ ì§œ ìƒì„± í•¨ìˆ˜
def generate_random_date():
    # 2025ë…„ ë‚´ì—ì„œ, ì˜¤ëŠ˜(2025-10-30)ì„ ë„˜ì§€ ì•Šë„ë¡ ì œí•œ
    start_date = datetime(2025, 1, 1)
    end_date = datetime(2025, 10, 30)
    time_between = end_date - start_date
    days_between = time_between.days
    random_days = random.randint(0, days_between)
    return (start_date + timedelta(days=random_days)).strftime("%Y-%m-%d")

# Dummy ë°ì´í„° ìƒì„±
dummy_data = {
    "Vector": [],
    "Model": [],
    "Search": [],
    "Database": [],
    "Framework": []
}

# ê° í‚¤ì›Œë“œë³„ë¡œ ëœë¤ ê°œìˆ˜ë§Œí¼ ìë£Œ ìƒì„±
paper_titles = [
    ("Vector Indexing for Semantic Search", "vector_semantic_search.pdf"),
    ("Machine Learning Models", "ml_models_guide.docx"),
    ("Neural Search Systems", "neural_search.pdf"),
    ("Vector Database Design", "vector_db_design.pdf"),
    ("Deep Learning Framework Comparison", "dl_framework.pdf"),
    ("Transformer Architecture Overview", "transformer_arch.pdf"),
    ("Hybrid Search Implementation", "hybrid_search.pdf"),
    ("Distributed Vector Storage", "dist_vector_storage.pdf"),
    ("LLM Framework Development", "llm_framework.pdf"),
    ("Semantic Embeddings", "semantic_embeddings.pdf"),
    ("BERT and GPT Models", "bert_gpt.pdf"),
    ("Information Retrieval Systems", "ir_systems.pdf"),
    ("NoSQL for AI Applications", "nosql_ai.pdf"),
    ("RAG Pipeline Framework", "rag_pipeline.pdf"),
    ("Vector Similarity Search", "vector_similarity.pdf"),
    ("Neural Information Extraction", "neural_extraction.pdf"),
    ("Graph Database Optimization", "graph_db.pdf"),
    ("AI Development Framework", "ai_dev_framework.pdf"),
    ("Semantic Search Algorithms", "semantic_algorithms.pdf"),
    ("Large Language Models", "llm_guide.pdf")
]

# í‚¤ì›Œë“œë³„ë¡œ ëœë¤ ê°œìˆ˜ì˜ ìë£Œ í• ë‹¹
keyword_list = list(dummy_data.keys())
title_idx = 0
for keyword in keyword_list:
    count = generate_random_count()
    for i in range(count):
        if title_idx >= len(paper_titles):
            title_idx = 0
        
        title, filename = paper_titles[title_idx]
        # ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” í•˜ìœ„ í‚¤ì›Œë“œ ì„ íƒ
        available_keywords = category_keywords.get(keyword, [])
        num_sub_keywords = min(random.randint(2, 3), len(available_keywords))
        sub_keywords = random.sample(available_keywords, num_sub_keywords)
        
        dummy_data[keyword].append({
            "ìë£Œëª…": title,
            "íŒŒì¼ëª…": filename,
            "ìƒì„±ì¼ì": generate_random_date(),
            "í•˜ìœ„í‚¤ì›Œë“œ": sub_keywords
        })
        title_idx += 1

# í‚¤ì›Œë“œë³„ ê¸°ìˆ  ê°œìˆ˜ ë°ì´í„° (ëœë¤)
keyword_counts = {
    "ì¹´í…Œê³ ë¦¬": list(dummy_data.keys()),
    "ê¸°ìˆ  ê°œìˆ˜": [len(data) for data in dummy_data.values()]
}

# 1ë¡œìš°: í‚¤ì›Œë“œ ê´€ê³„ë„ 3, ìƒì„¸í†µê³„ 1
col_graph, col_stats = st.columns([3, 1])

with col_graph:
    st.markdown("#### ğŸ“Š Document Hub Graph")
    
    # ë…¸ë“œì™€ ì—£ì§€ ìƒì„±
    nodes = []
    edges = []
    
    # Document Hub ì¤‘ì‹¬ ë…¸ë“œ ì¶”ê°€
    nodes.append(
        Node(
            id="Document Hub",
            label="Document Hub",
            size=50,
            color="#FF0000",
            font={"size": 24, "face": "Arial", "color": "white"},
            shape="dot",
            title="Document Hub"
        )
    )
    
    # ë©”ì¸ í‚¤ì›Œë“œë¥¼ ì¹´í…Œê³ ë¦¬ ë…¸ë“œë¡œ ì¶”ê°€
    main_keywords = list(dummy_data.keys())
    
    for keyword in main_keywords:
        nodes.append(
            Node(
                id=keyword,
                label=keyword,
                size=35,
                color="#FF6B6B",
                font={"size": 16, "face": "Arial", "color": "white"},
                shape="dot",
                title=keyword
            )
        )
        # Document Hubì™€ ì¹´í…Œê³ ë¦¬ ë…¸ë“œ ì—°ê²°
        edges.append(
            Edge(
                source="Document Hub",
                target=keyword,
                color="#FFFFFF",
                width=3
            )
        )
    
    # í•˜ìœ„í‚¤ì›Œë“œ ìˆ˜ì§‘ ë° ë…¸ë“œ ì¶”ê°€
    all_sub_keywords_set = set()
    sub_keyword_to_main = {}  # í•˜ìœ„í‚¤ì›Œë“œê°€ ì–´ë–¤ ë©”ì¸ í‚¤ì›Œë“œì— ì†í•˜ëŠ”ì§€ ì¶”ì 
    
    for keyword, papers in dummy_data.items():
        for paper in papers:
            for sub_keyword in paper['í•˜ìœ„í‚¤ì›Œë“œ']:
                all_sub_keywords_set.add(sub_keyword)
                if sub_keyword not in sub_keyword_to_main:
                    sub_keyword_to_main[sub_keyword] = []
                sub_keyword_to_main[sub_keyword].append(keyword)
    
    # í•˜ìœ„í‚¤ì›Œë“œë¥¼ ë©”ì¸ í‚¤ì›Œë“œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
    main_keyword_colors = {
        "Vector": "#4ECDC4",
        "Model": "#FFA07A",
        "Search": "#98D8C8",
        "Database": "#F7DC6F",
        "Framework": "#BB8FCE"
    }
    added_nodes = set()  # (sub_keyword, main_keyword) íŠœí”Œë¡œ ì¶”ì 
    
    for sub_keyword, main_keywords_list in sub_keyword_to_main.items():
        for main_keyword in main_keywords_list:
            # (í•˜ìœ„í‚¤ì›Œë“œ, ë©”ì¸í‚¤ì›Œë“œ) ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            if (sub_keyword, main_keyword) not in added_nodes:
                nodes.append(
                    Node(
                        id=f"{sub_keyword}_{main_keyword}",
                        label=sub_keyword,
                        size=15,
                        color=main_keyword_colors.get(main_keyword, "#85C1E2"),
                        font={"size": 11, "face": "Arial", "color": "white"},
                        shape="dot",
                        group=main_keyword  # ê·¸ë£¹ ì§€ì •
                    )
                )
                added_nodes.add((sub_keyword, main_keyword))
            
            edges.append(
                Edge(
                    source=main_keyword,
                    target=f"{sub_keyword}_{main_keyword}",
                    color=main_keyword_colors.get(main_keyword, "#CCCCCC"),
                    width=1.5
                )
            )
    
    # ê·¸ë˜í”„ ì„¤ì •
    config = Config(
        width="100%",
        height=500,
        directed=False,
        hierarchical=False,
        layout={
            "hierarchical": False,
            "improvedLayout": True
        },
        node={
            "labelProperty": "label",
            "font": {"size": 12}
        },
        edge={
            "smooth": {
                "enabled": True, 
                "type": "curvedCW",
                "roundness": 0.3
            },
            "length": 100,  # ì§§ê²Œ ì„¤ì •
            "width": 2
        },
        # ë…¸ë“œ ê°„ ê±°ë¦¬ ì¡°ì • (ê³„ì¸µ êµ¬ì¡°: Data Hub -> ì¹´í…Œê³ ë¦¬ -> í•˜ìœ„í‚¤ì›Œë“œ)
        physics={
            "enabled": True,
            "stabilization": True,
            "barnesHut": {
                "gravitationalConstant": -500,
                "centralGravity": 0.0,
                "springLength": 60,
                "springConstant": 0.15,
                "damping": 0.2,
                "avoidOverlap": 1
            }
        },
        # ë…¸ë“œ ì„ íƒ ê¸°ëŠ¥
        interaction={
            "hover": True,
            "selectConnectedEdges": True,
            "selectConnectedNodes": True,
            "tooltipDelay": 300,
            "dragNodes": True,
            "dragView": True
        }
    )
    
    # í‚¤ì›Œë“œë³„ ê¸°ìˆ  ê°œìˆ˜ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    chart_df = pd.DataFrame(keyword_counts)
    
    # ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ í•„í„°ë§
    if 'selected_category' in st.session_state and st.session_state.selected_category:
        # ì„ íƒëœ ì¹´í…Œê³ ë¦¬ë§Œ í‘œì‹œ
        filtered_nodes = []
        filtered_edges = []
        
        # Document HubëŠ” í•­ìƒ í‘œì‹œ
        for node in nodes:
            if node.id == "Document Hub":
                filtered_nodes.append(node)
            elif node.id == st.session_state.selected_category:
                filtered_nodes.append(node)
            elif hasattr(node, 'group') and node.group == st.session_state.selected_category:
                filtered_nodes.append(node)
        
        # í•„í„°ë§ëœ ë…¸ë“œì— ì—°ê²°ëœ ì—£ì§€ë§Œ í‘œì‹œ
        node_ids = {node.id for node in filtered_nodes}
        for edge in edges:
            # Edge ê°ì²´ì˜ ì†ì„± ì ‘ê·¼
            edge_source = getattr(edge, 'source', None)
            edge_target = getattr(edge, 'target', None)
            if edge_source in node_ids or edge_target in node_ids:
                filtered_edges.append(edge)
        
        nodes = filtered_nodes
        edges = filtered_edges
    
    # ê·¸ë˜í”„ ë Œë”ë§
    try:
        agraph(nodes=nodes, edges=edges, config=config)
    except Exception as e:
        st.error(f"ê·¸ë˜í”„ ë Œë”ë§ ì˜¤ë¥˜: {e}")
        st.info("ì¼ë°˜ ì°¨íŠ¸ë¡œ ëŒ€ì²´ í‘œì‹œí•©ë‹ˆë‹¤.")
        st.bar_chart(chart_df.set_index("ì¹´í…Œê³ ë¦¬"))

with col_stats:
    st.markdown("#### ğŸ“ˆ ì¹´í…Œê³ ë¦¬ í†µê³„")
    st.dataframe(
        chart_df,
        use_container_width=True,
        hide_index=True
    )
    
    # ê·¸ë˜í”„ ì¹´í…Œê³ ë¦¬ ì„ íƒ
    st.markdown("#### ğŸ” ì¹´í…Œê³ ë¦¬ í•„í„°")
    filter_options = ["ì „ì²´"] + list(dummy_data.keys())
    
    if 'graph_category_filter' not in st.session_state:
        st.session_state.graph_category_filter = "ì „ì²´"
    
    selected_filter = st.selectbox(
        "í‘œì‹œí•  ì¹´í…Œê³ ë¦¬",
        filter_options,
        key="graph_filter",
        help="ê·¸ë˜í”„ì— í‘œì‹œí•  ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”"
    )
    
    # í•„í„° ì„ íƒ ì‹œ ì—…ë°ì´íŠ¸
    if selected_filter != st.session_state.graph_category_filter:
        st.session_state.graph_category_filter = selected_filter
        if selected_filter == "ì „ì²´":
            st.session_state.selected_category = None
        else:
            st.session_state.selected_category = selected_filter
        st.rerun()

st.markdown("---")

# 2ë¡œìš°: ë…¼ë¬¸ ëª©ë¡
st.markdown("#### ğŸ“‹ Document Hub ëª©ë¡")

# ì¹´í…Œê³ ë¦¬ ì„ íƒ ë“œë¡­ë‹¤ìš´
keyword_options = ["ì „ì²´"] + list(dummy_data.keys())

# ë…¼ë¬¸ ëª©ë¡ í‚¤ì›Œë“œ í•„í„° ì´ˆê¸°í™”
if 'paper_keyword_filter' not in st.session_state:
    st.session_state.paper_keyword_filter = "ì „ì²´"

# ê¸°ë³¸ ì¸ë±ìŠ¤ ê³„ì‚°
if st.session_state.paper_keyword_filter in keyword_options:
    default_idx = keyword_options.index(st.session_state.paper_keyword_filter)
else:
    default_idx = 0

selected_keyword = st.selectbox(
    "ì¹´í…Œê³ ë¦¬ ì„ íƒ",
    keyword_options,
    index=default_idx,
    help="ì „ì²´ ë˜ëŠ” íŠ¹ì • ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì—¬ ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”",
    key="keyword_selectbox"
)

# ì„ íƒëœ ê°’ì´ ë³€ê²½ë˜ë©´ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
if selected_keyword != st.session_state.paper_keyword_filter:
    st.session_state.paper_keyword_filter = selected_keyword

# í•„í„°ë§ëœ ë°ì´í„°
if selected_keyword == "ì „ì²´":
    filtered_data = []
    for keyword, papers in dummy_data.items():
        for paper in papers:
            filtered_data.append({
                "ì¹´í…Œê³ ë¦¬": keyword,
                **paper
            })
else:
    filtered_data = []
    for paper in dummy_data[selected_keyword]:
        filtered_data.append({
            "ì¹´í…Œê³ ë¦¬": selected_keyword,
            **paper
        })

# í‘œë¡œ í‘œì‹œ
if filtered_data:
    # í‘œìš© ë°ì´í„°í”„ë ˆì„ ìƒì„±
    table_data = []
    for item in filtered_data:
        sub_keywords_str = ", ".join(item['í•˜ìœ„í‚¤ì›Œë“œ'])
        table_data.append({
            "ì¹´í…Œê³ ë¦¬": item['ì¹´í…Œê³ ë¦¬'],
            "ìë£Œëª…": item['ìë£Œëª…'],
            "íŒŒì¼ëª…": item['íŒŒì¼ëª…'],
            "ìƒì„±ì¼ì": item['ìƒì„±ì¼ì'],
            "í•˜ìœ„í‚¤ì›Œë“œ": sub_keywords_str
        })
    
    table_df = pd.DataFrame(table_data)
    
    # í˜ì´ì§€ë„¤ì´ì…˜ ì„¤ì •
    items_per_page = 10
    total_pages = (len(table_df) + items_per_page - 1) // items_per_page
    
    # í˜„ì¬ í˜ì´ì§€ ìƒíƒœ ê´€ë¦¬
    page_key = f"page_{selected_keyword}"
    if page_key not in st.session_state:
        st.session_state[page_key] = 1
    
    current_page = st.session_state[page_key]
    
    # í˜„ì¬ í˜ì´ì§€ ë°ì´í„° í‘œì‹œ
    start_idx = (current_page - 1) * items_per_page
    end_idx = start_idx + items_per_page
    
    # í‘œìš© ë°ì´í„°í”„ë ˆì„ ìƒì„±
    display_data = []
    for item in filtered_data[start_idx:end_idx]:
        keywords_text = " ".join([f"[{keyword}]" for keyword in item['í•˜ìœ„í‚¤ì›Œë“œ']])
        display_data.append({
            "ì¹´í…Œê³ ë¦¬": item['ì¹´í…Œê³ ë¦¬'],
            "ìë£Œëª…": f"ğŸ“„ {item['ìë£Œëª…']}",
            "íŒŒì¼ëª…": f"ğŸ“‚ {item['íŒŒì¼ëª…']}",
            "ìƒì„±ì¼ì": f"ğŸ“… {item['ìƒì„±ì¼ì']}",
            "í•˜ìœ„í‚¤ì›Œë“œ": keywords_text
        })
    
    # í•­ìƒ 10ê°œ í–‰ìœ¼ë¡œ ê³ ì • (ë¶€ì¡±í•˜ë©´ ë¹ˆ í–‰ ì¶”ê°€)
    while len(display_data) < items_per_page:
        display_data.append({
            "í‚¤ì›Œë“œ": "",
            "ìë£Œëª…": "",
            "íŒŒì¼ëª…": "",
            "ìƒì„±ì¼ì": "",
            "í•˜ìœ„í‚¤ì›Œë“œ": ""
        })
    
    display_df = pd.DataFrame(display_data)
    
    # í‘œë¡œ í‘œì‹œ
    st.dataframe(
        display_df,
        use_container_width=True,
        hide_index=True
    )
    
    # í˜ì´ì§€ ì„ íƒ ë²„íŠ¼ (í‘œ ì•„ë˜ì— ë°°ì¹˜, ì˜¤ë¥¸ìª½ ì •ë ¬)
    col_left, col_right = st.columns([6, 2])
    with col_left:
        st.caption(f"ì´ {len(filtered_data)}ê°œ | í˜ì´ì§€: {current_page}/{total_pages}")
    with col_right:
        # ì´ì „/ë‹¤ìŒ ë²„íŠ¼ì„ í•œ ì¤„ì— ë°°ì¹˜
        col_prev, col_next = st.columns(2)
        with col_prev:
            if total_pages > 1:
                prev_disabled = current_page == 1
                if st.button("â—€ ì´ì „", disabled=prev_disabled, key=f"prev_{page_key}", use_container_width=True):
                    st.session_state[page_key] = max(1, current_page - 1)
                    st.rerun()
        with col_next:
            if total_pages > 1:
                next_disabled = current_page >= total_pages
                if st.button("ë‹¤ìŒ â–¶", disabled=next_disabled, key=f"next_{page_key}", use_container_width=True):
                    st.session_state[page_key] = min(total_pages, current_page + 1)
                    st.rerun()
else:
    st.info("ì„ íƒí•œ í‚¤ì›Œë“œì— ëŒ€í•œ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤.")